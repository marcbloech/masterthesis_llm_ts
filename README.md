# Master Thesis Repository: One Taxonomy is All You Need

This repository contains the data, code, and results associated with my Master's thesis. The research focuses on evaluating the capabilities of LLMs in Text Simplification (TS), and ultimately yields a proposed error taxonomy for TS with LLMs.

## Repository Structure

The repository is organized into two main directories: `data` and `notebooks`.

### 1. `data/`

This directory houses all the datasets, annotations, model inputs/outputs, and intermediate results used in the thesis.

*   **`LLM_annotations/`**: Contains annotations generated by various Large Language Models (LLMs)
*   **`LLM-as-a-judge_outputs/`**: Stores results from experiments where LLMs were used to evaluate or rank text simplification outputs (e.g., pairwise preference judgments).
*   **`salsa_annotations/`**: Holds TS annotations performed using the SALSA framework
    *   `final_annotated/`: Contains the finalized, curated human annotations for outputs from different TS systems across various datasets (Cochrane, SCOTUS, WikiDE, WikiEN). Also includes aggregated dataframes that have been transformed.
*   **`salsa_peer_annotations/`**: Contains data related to the inter-annotator agreement study
    *   `final_peer_annotations/`: Individual annotator files
    *   `salsa_peer_data_wikiDE_random.json`: The specific data subset used for the peer annotation task.
*   **`tax_validation_annotations/`**: Stores human annotations specifically focused on validating the proposed taxonomy.
*   **`TS_datasets/`**: Contains the original source datasets used for the text simplification tasks.
    *   `asset-main/`: The ASSET benchmark dataset
    *   `TextComplexityDE-master/`: A German text complexity dataset.
    *   The other `.csv` files represent custom-curated datasets from various domains (Cochrane, SCOTUS, WikiEN, WikiDE). Also includes the specific subset used for taxonomy validation (`taxonomy_validation_subset_n50.csv`).
*   **`TS_model_outputs/`**: Contains the simplified text generated by various text simplification models.


### 2. `notebooks/`

This directory contains the Jupyter notebooks and Python scripts used for analysis, data processing, simplification generation, and LLM interactions.

*   **`prompts_LLM_annotations/`**: Stores the text files containing the specific prompts used to elicit annotations or judgments from LLMs.
*   **`analyze_annotation_preferenceRanking.ipynb`**: Notebook for analyzing the results of LLM-as-a-judge preference ranking experiments.
*   **`analyze_interAnnotator_agreement.ipynb`**: Notebook for calculating and analyzing inter-annotator agreement scores.
*   **`analyze_operations.ipynb`**: Notebook for analyzing the edit operations and error patterns.
*   **`generate_LLM_annotations.ipynb`**: Notebook containing the code to interact with LLM APIs and generate the annotations stored in `data/LLM_annotations/`.
*   **`generate_simplications.ipynb`**: Notebook used to run various text simplification models and generate the outputs stored in `data/TS_model_outputs/`.
*   **`helper_functions.py`**: A Python script containing reusable functions used across different notebooks.
*   **`transform_annotations_functions.py`**: A Python script containing functions specifically designed for processing, converting, or transforming annotation data (e.g., parsing SALSA format).
*   **`transform_annotations_pipeline.ipynb`**: Notebook orchestrating the annotation transformation process.

## How to Use

1.  Explore the `data/` directory to understand the datasets and annotations generated/used.
2.  Examine the notebooks in the `notebooks/` directory to understand the methodology for simplification generation, annotation (human and LLM), and analysis.